# -*- coding: utf-8 -*-
"""URandom-forest-regressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d9Vwq62j4fCgdfyWx1iN-RdXqbOg7s6A
"""

"""
Author: Zoe, UTAT -SS - Science

This code uses Random forest for the unmixing process. Sources: https://www.geeksforgeeks.org/random-forest-regression-in-python/, https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/

WARNING: TAKES SUPER LONG. This code includes hyperparameter tuning, which means that it can take like 10-15 minutes to tun. If you don't want to wait this long, get rid of the hyperparameter tuning section and instead just pick them (or take the default ones)

With simpler data only, Mean Squared Error: 0.026416158563140196
R-squared: 0.7257821483336598

More information on random forest on the notion: https://www.notion.so/utat-ss/FAE-Random-Forest-1b63e028b0ea80e3afcad34492232512

Here, it is used on simpler_data. Make sure to change the filepath to your computer's! (search for "#put your data here!"

"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

"""Importing and cleaning data set"""
# Load dataset
ds = pd.read_csv("/Users/zoe/Downloads/simpler_data.csv") #put your data here!

# Drop label and wavelengths that are beyond FINCH's
ds = ds.drop(columns='Spectra')
ds = ds.drop(ds.iloc[:, 3:54], axis=1)
ds = ds.drop(ds.iloc[:, 84:10000], axis=1)

"""Defining features and vairables, splitting dataset"""

# Define features (x) and target variables (y)
x = ds.iloc[:, 3:1000]
y = ds[['gv_fraction', 'npv_fraction', 'soil_fraction']]

# Split the dataset into training and testing sets (80% train, 20% test, can change it
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)


"""Hyperparameter tuning: this makes the computation take forever (10-15 min in my computer lol). I haven't put that much into trying to optimize it,since there's not really any need to run it super often. What this basically does is it runs  through all possible combinations of parameters provided in the param_grid and keeps the best one. This means that it's doing a bunch of random forests.


n_estinmators: number of trees considered. More trees usually give better results, but take longer
max_depth: Controls the maximum depth of each tree. A shallow tree is prone to underfitting and a deep tree may overfit.
min_sample_split: minimum # of samples required to split an internal node.
min_samples_leaf: minimum # of samples required to be at a leaf node
max_features: Limits the number of features to consider when splitting a node, which helps control overfitting.


Not being used: max_leaf_nodes: Limits the number of leaf nodes in the tree (controls its size and complexity). For now, making it lighter is not such a big problem.
There are a couple more.
"""

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2']
}

# trying the hyperparameters combinations
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=5,  # 5-fold cross-validation
                           n_jobs=-1,  # Use all available CPU cores
                           verbose=0)  #otherwise it prints everything

# Fit the grid search on the training data
grid_search.fit(x_train, y_train)

# Print the best hyperparameters found
print("\nBest parameters found by GridSearchCV:")
print(grid_search.best_params_)

# Get the best model
best_model = grid_search.best_estimator_

# Predict using the best model
predictions = best_model.predict(x_test)


""" Evaluate the model """

mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f"\nMean Squared Error: {mse}")
print(f"R-squared: {r2}")