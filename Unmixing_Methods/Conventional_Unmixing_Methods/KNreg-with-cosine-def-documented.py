# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hootLVh4u_Wpcoy57OzuizVGx13wPVO6
"""

"""
Author: Zoe, UTAT -SS - Science

This code runs a K-nearest neighbors regression for the unmixing process. It is based on distance, and is an intrapolation based on publicly available data, in this case simpler_data.

To run this code:
 - Change the data set file path (look for "#change to users' file path"
 - You may want to try different definitions of distance. Here, we use the cosine definition because it worked best for simpler_data. If you want to try a different one, look for "#change to switch the distance difinition" (it appears twice)

 General information on how KN-regressor and the cosine definition of distance work, as well as recommended background readings and potential limitations can be found on the notion page:
 Additionally, it might be useful to read about the K-Nearest Neighbors classifier to undertsand the function better: https://utat-ss.notion.site/Endmember-Classification-Report-KNN-and-KMC-1963e028b0ea803da059c48a931f3e21


 Because KNN-regressor is a regressor, no endmembers can be extracted.


Broadly speaking the algorithm does the following:
1)import libraries and the file
2) drop unused labels and wavelengths that are outside of FINCH's range from the data file
3) Split into testing and training data
4) Circle through k-values to find the best one: The k value determines how many neighbors will be taken into account and averaged to predict the relative abundances from a spectrum (mor einformation on the notion). Becuase there is no way of determining the best one beforehand,. we circle through a bunch of them and keep the one that leads to the smallest R2 value. The R2 value is obtained by plotting expected vs real relative abundances per material (eg. npv, soil, gv) per point.
5) With the best k-value, train the final model
6) Calculate regression metrics (eg. MSE and MAE), R2 from predicted vs actual values, etc.
7) Look at how far the sum of the relative abundances form each pointis from q: This akgorithm does not limit the values the abundances or their sum can take, meaning gv+soil+npv can be bigger or smaller than one. This could potentially be a problem because relative abundances that are above or below 1 have no real-world meaning. Here, we look at the max and min values for the sum of the relative abundances and its mean and standard deviation. If KN-regressor proves to be useful, we should limit the sum of the abundances per point to be 1.

"""



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

import statistics


# Load dataset
ds = pd.read_csv("/Users/zoe/Downloads/simpler_data.csv")  #change to users' file path


# Drop label and wavelengths that are beyond FINCH's
ds = ds.drop(columns='Spectra')
ds = ds.drop(ds.iloc[:, 3:54], axis=1)
ds = ds.drop(ds.iloc[:, 84:10000], axis=1)

#print(ds.columns)

# Define features (X) and target variable (y)

X = ds.iloc[:, 3:1000]
y = ds[['gv_fraction', 'npv_fraction', 'soil_fraction']]


# Spliting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)




"""KNN regression: Loop through k values to find the biggest R2 score """

max_r2 = -float("inf")
best_k = None
r2_results = {}

for k in range(1, 12):
    # Train KNN regressor for current k
    knn = KNeighborsRegressor(n_neighbors=k, metric='cosine') #change to switch the distance difinition
    knn.fit(X_train, y_train)

    # Make predictions
    y_pred = knn.predict(X_test)

    # Calculate the R² score
    current_r2 = r2_score(y_test, y_pred)
    r2_results[k] = current_r2

    # Check if this is the biggest R² so far
    if current_r2 > max_r2:
        max_r2 = current_r2
        best_k = k

    #print(f"k = {k}, R² Score = {current_r2:.4f}")


print(f"The k value with the biggest R² is {best_k} with an R² of {max_r2:.4f}")




"""use best k-value to test final model """

knn_best = KNeighborsRegressor(n_neighbors=best_k, metric='cosine') #change to switch the distance difinition
knn_best.fit(X_train, y_train)
y_pred_best = knn_best.predict(X_test)




"""Calculate regression metrics for the best model"""

mse = mean_squared_error(y_test, y_pred_best)
mae = mean_absolute_error(y_test, y_pred_best)
r2 = r2_score(y_test, y_pred_best)

print(f'Mean Squared Error (MSE): {mse:.4f}')
print(f'Mean Absolute Error (MAE): {mae:.4f}')
print(f'R² Score for best k ({best_k}): {r2:.4f}')

# Plotting Actual vs Predicted for each target variable
plt.figure(figsize=(10, 6))
plt.scatter(y_test['gv_fraction'], y_pred_best[:, 0], alpha=0.5, label="gv_fraction")
plt.scatter(y_test['npv_fraction'], y_pred_best[:, 1], alpha=0.5, label="npv_fraction")
plt.scatter(y_test['soil_fraction'], y_pred_best[:, 2], alpha=0.5, label="soil_fraction")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.legend()
plt.title("Actual vs Predicted Values")
plt.show()



"""Testing how far the sum of the relative abundances is from 1"""
#test whether the abundance for npv/gv/soil individually are always between o and 1
print(y.min())
print(y.max())


#look at the points' mean and standard deviation, since we didn't limit it by saying that each individual abundance was between 0 and 1 and they all had to sum up to 1
surfacessummed=sum_across_rows = np.sum(y_pred_best, axis=1)
print ("The standard deviation of the sum of the relative abundances is : ",end="")
print (statistics.stdev(surfacessummed))
print ("The mean of the sum of the relative abundances is : ",end="")
print (statistics.mean(surfacessummed))